---
title: "Predicting Blood Donations"
author: "Lila Jomok"
date: "November 14, 2017"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: spacelab
    variant: markdown_github
---

# About
In addition to refreshing data analysis skills, our goal is to build a statistical model to predict if a blood donor will donate within a given time window, which is March 2007. 

# Data
The provided datasets, `trainingData.csv` and `testData.csv` contain the following variables:

- `X1`: ID of donor.
- `Months since Last Donation`: Number of months since the donor's most recent blood donation.
- `Number of Donations`: Total number of donations the donor has made.
- `Total Volume Donated`: Total amount of blood the donor has donated in cubic centimeters.
- `Months since First Donation`: Number of months since the donor's first blood donation.
- `Made Donation in March 2007`: The explanatory variable or result - `1` if they donated blood, `0` if they did not donate blood in March 2007.

We can view the first couple of observations of the datasets below:
```{r, message=FALSE, warning=FALSE}
# Set working directory and load packages
getwd()
library(tidyverse)
library(gridExtra)
library(aod)
library(corrplot)

# Import datasets; adjust if datasets are in a different directory
testData = read.csv("data/testData.csv", header = TRUE)
trainingData = read.csv("data/trainingData.csv", header = TRUE)

# Display first rows of datasets
head(trainingData)
head(testData)
```
Since `testData` is our testing data set, it does not have `Made Donation in March 2007` variable.

## Renaming Variables
To make the data analysis more simple, we will rename the variables using `colnames()`:
```{r, message=FALSE, warning=FALSE}
# Rename variables in testData and trainingData
colnames(testData) <- c("ID", "mosLastDo", "numDonations", "totVol", "mosFirstDo")
colnames(trainingData) <- c("ID", "mosLastDo", "numDonations", "totVol", "mosFirstDo", "madeDonation")

# Display first rows of datasets (to confirm name changes)
head(trainingData)
head(testData)
```

# Exploratory Data Analysis
Since we are interested in whether the donor will donate again in March 2007, there are two possible outcomes: either they donate or do not donate. A logistic regression model is suitable for this, but we will first do some exploratory data analysis. Having a visual idea of the data will help us see if we need to do some transformations and see possible problems with our dataset.

First, we need to make the binary factor `madeDonation` is read as a categorical variable. Otherwise, the `0` and `1` values will be read as continuous variables instead of two groups.  
```{r, message=FALSE, warning=FALSE}
# Turn madeDonation into factors instead of continuous
trainingData$madeDonation <- factor(trainingData$madeDonation)
contrasts(trainingData$madeDonation)

# trainingData$ID <- factor(trainingData$ID)
```

Summary of `trainingData`:
```{r}
summary(trainingData)
```

## Boxplots

We plot Boxplots of the two groups to see if there are any patterns:
```{r}
# Boxplots
plot01 <- ggplot(data = trainingData) + geom_boxplot(aes(x = madeDonation, y = numDonations)) + labs(title = "Number of Donations", subtitle = "Made Donations vs. No Donations")
plot02 <- ggplot(data = trainingData) + geom_boxplot(aes(x = madeDonation, y = totVol)) + labs(title = "Total Volume Donated", subtitle = "Made Donations vs. No Donations")
plot03 <- ggplot(data = trainingData) + geom_boxplot(aes(x = madeDonation, y = mosLastDo)) + labs(title = "Months since Last Donation", subtitle = "Made Donations vs. No Donations")
plot04 <- ggplot(data = trainingData) + geom_boxplot(aes(x = madeDonation, y = mosFirstDo)) + labs(title = "Months since First Donation", subtitle = "Made Donations vs. No Donations")
grid.arrange(plot01, plot02, plot03, plot04, ncol = 2)
```

`numDonations` and `totVol` look similar, which makes sense - the total amount of blood donated *should* increase as the number of donations increase. We can see this by looking at their correlation:
```{r}
TDselect <- select(trainingData, mosLastDo, numDonations, totVol, mosFirstDo)
cor1 <- round(cor(TDselect), 2)
cor1
```

Both `numDonations` and `totVol` have a correlation of `1.00`. Hence, we can leave out `totVol` in the model.

## Feature Engineering

If we create a new variable that takes the average of each donation (`totVol` / `numDonations`), each observation will be the same - 250 cubic centimeters. However, the number of donations varies among the donors, so we will create a variable `donFreq` which is the length of time between a donor's first and last donation divided by the total number of donations.

We can also create a new binary variable `firstTime` to indicate which donors are donating for the first time.
```{r}
TD2 <- mutate(trainingData, donFreq = (mosFirstDo - mosLastDo) / numDonations)
TD2 <- mutate(TD2, firstTime = ifelse(mosFirstDo == mosLastDo, "0", "1"))
TD2$firstTime <- factor(TD2$firstTime) # turn into factors
head(TD2)
```

# Logistic Regression Analysis

We will consider a simple logistic linear model for our data.
```{r}
model01 <- glm(madeDonation ~ mosLastDo + numDonations + mosFirstDo + donFreq + firstTime, family = binomial(link = 'logit'), data = TD2)
summary(model01)
```

At the significance level of alpha = 0.05, `mosLastDo` and `firstTime` are statistically significant.

- For every unit change in `mosLastDo`, the log odds of donating in March 2007 reduces by `0.09708`.
- If the donor is a NOT first-time donor (`firstTime1`), the log odds of donating in March 2007 changes by `1.29274`.

## Wald test for `firstTime`

We will perform a Wald test the overall effectiveness of `firstTime`.
```{r}
wald.test(b = coef(model01), Sigma = vcov(model01), Terms = 5)
```

The chi-squared statistic of `2.7` with `1` degrees of freedom and a p-value of `0.098` indicates that the overall effect of `firstTime` is statistically insignificant.

## Odds Ratios

We can obtain the odds-ratio and 95% CI from the coefficients:
```{r}
exp(cbind(OR = coef(model01), confint(model01)))
```

For a unit increase in `mosLastDo`, the odds of donating in March 2007 increases by `0.9074839`.

# Goodness of Fit

Before we can start predicting with our model, we will test the Full and Reduced models with a Likelihood Ratio Test.
```{r}
# Create reduced model
model02 <- glm(madeDonation ~ mosLastDo + firstTime, family = binomial(link = 'logit'), data = TD2)
summary(model02)

# Likelihood Ratio test bt. Full and Reduced
anova(model01, model02, test = "LRT")
```
A p-value of `2.569e-06` indicates that the full model is a better fit than the reduced model. Compared to the reduced model, the full model's AIC is slightly lower (`556.29` to `579.01`).

# Predictions

Before we can start predicting, we need to adjust `testData` with the same variables as `trainingData`.
```{r}
testData2 <- mutate(testData, donFreq = (mosFirstDo - mosLastDo) / numDonations)
testData2 <- mutate(testData2, firstTime = ifelse(mosFirstDo == mosLastDo, "0", "1"))
testData2$firstTime <- factor(testData2$firstTime)
head(testData2)
```

We will create a new data frame `donateP` to store our predicted probabilities in.
```{r}
# Create a base DF to be used in predictions
newDF <- with(testData2, data.frame(ID = ID, mosLastDo = mosLastDo, numDonations = numDonations,  mosFirstDo = mosFirstDo, donFreq = donFreq, firstTime = factor(firstTime)))
# head(newDF)

newDF$donateP <- predict(model01, newdata = newDF, type="response")
head(newDF)
```

## Submission
After predictions, we will then use `dplyr` to create another data frame for our submission, which only includes the donor's `ID` and the predicted probability. 
```{r}
submissionData <- select(newDF, ID, donateP)
colnames(submissionData) = c("X1", "Made Donation in March 2007") # rename columns to submission format

# Save submissionData as CSV format
write.csv(submissionData, file = "submissionData.csv", row.names = FALSE)

head(submissionData)
```

With our submission file created and exported, we are about done! All that is left is to submit it! 

# Future Updates

After the first submission, the next step in this project is to see whether we can improve the performance of our model.
